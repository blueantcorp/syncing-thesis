
\chapter{Background}
\label{sec:background}
We will start this chapter by explaining the core aspects of data synchronization.
After setting it in context with traditional properties of distributed databases we will present some of the popular approaches to synchronization.\\
Some technical background on the practicality of local data storage based on HTML5 will follow.\\
This will give us a solid background to develop and reason about our own data synchronization framework.

\section{Defining Data Synchronization}
Let us introduce some basic terminology and try to systematically define what data synchronization is about.

\emph{Atoms} are what we define as the lowest level of data that cannot be devided into smaller parts.
Every application may have a custom definition of atoms.
For a file synchronizer it may be entire files, for a source code management system it may be lines in a file, for a collaborative task manager it may be literal values like strings, numbers or dates.\\
Atoms can be aggregated to \emph{objects}.
A source code management system may define objects as a sequence of lines aggregated to a file.
The task manager could aggregate values like strings and dates to task objects.
Objects can themselves be aggregated further into larger objects.
File objects can be combined to directory objects, task objects into larger structures like a project.
Aggregation of objects can also be seens as relationships between objects.\\
A collaborative application has multiple users working on different devices on a related set of data.
They are either connected directly or via servers who live on the local network or the Internet.
Each device, be it a user's device or a server, we define as a \emph{node}.
Nodes can be connected through various network topologies like peer-to-peer, client-server or a hierarchical architecture.
In section \ref{sec:main.requirements.topologies} we will go into more detail about different network topologies.\\
The nodes of mobile users are likely to be partitioned from their network and therefore have to be able to work in \emph{offline} mode.
Therefore application data has to be available locally so that users are not blocked from using their application.
Even when connected to a network it can be beneficial to maintain data locally to increase the responseness of the application.
In a collaborative application, edits that are made locally will have to be \emph{synchronized} with other participating nodes.\\
The process of synchronization can be divided into three phases.
Local edits first have to be identified before they can be sent to other nodes.
We refer to this step as the \emph{update detection} phase.
Some applications may explicitly track each edit as its made and store the history of edit operations.
This \emph{edit-based} approach is necessary for \emph{stream-based synchronization} which we explain in section \ref{sec:background.stream-based}.\\
If edits are not tracked directly we have to run a differencing algorithm to detect updates.
This requires us to keep previous states of the data and is detailed further in section \ref{sec:background.history-based} on \emph{history-based synchronization}.\\
Once updates are detected we continue with the \emph{update propagation} phase.
A stream of edit operations or the differencing output is sent to the collaborating nodes.
The details will be explained in the respective sections on the stream or history based approaches.\\
In a final phase the received data has to be \emph{reconceiled} with the local data on each node.
Updates have to be merged and conflicts are identified.
In a centralized scenario this part is usually carried out by the server.
Distributed architectures supporting peer-to-peer synchronization are much more complex as all clients have to reconcile the received updates in an eventually consistent way.\\

Summarizing these terms:

\begin{itemize}
\item \textbf{Atoms} are the literal values that can not be divided further.
\item \textbf{Objects} aggregate atoms or other objects into larger structures.
\item \textbf{Nodes} are the collaborating devices in an application.
\item The \textbf{update detection} phase identifies local data changes on each node.
\item During \textbf{update propagation} changed data is sent to collaborating nodes.
\item \textbf{Reconciliation} merges data received from other nodes and identifies conflicts.
\end{itemize}

\emph{Update detection}, \emph{update propagation} and \emph{update reconciliation} combined are what we define as \emph{data synchronization}.

\section{ACID Properties and Transactions}
The nodes of a collaborative, mobile application with replicated data represent a distributed database.
Distributed database systems have been a focus of research for decades.
Traditionally, the incentive to make databases distributed has been to provide fault tolerance, increase read/write throughput or to increase storage capacities.
Mobile applications need data replication to reduce frequent network access resulting in a better user experience.
Traditional distributed databases used to back enterprise applications running entirely in server farms.
Servers are connected through reliable and high-speed networks.
Network partitions are the absolute exception.\\
On mobile devices network partitions or slow connections are the norm.
Users want to work with their notebooks even when not being in an office environment with reliable Internet access.
Mobile networks are still comparatively slow and unreliable.\\
Back in 1981 Jim Gray defined the properties of a reliable transaction system \cite{Gray:1981wi}.
They are referred to as the ACID (Atomicity, Consistency, Isolation, Durability) properties - a term coined by Andreas Reuter and Theo HÃ¤rder \cite{Reuter1983}.
Consistency is defined as the property ensuring that a database can only transition between valid states.
One way to achieve this is to use locking so that a record can not be edited concurrently.
In an always-connected server environment transactions are measured in seconds - locking of data can therefore be acceptable.
In a mobile setting this is not an option as transactions can easily last days.
A mobile user who wants to edit some data while travelling without network connection should certainly not block all other users from doing their work.
Using locking concepts in such a scenario would not only be a an inconvenience for the users but would actually lead to a high-rate of deadlocks.
As Jim Gray states, the rate of deadlocks goes up with the square of the level of concurrency and the fourth power of the transaction size \cite{Gray3}.\\
A common alternative to locking is \emph{multiversion concurrency-control} where readers can still access the prior version of data being editd by another user.
Distributed databases usually use a two-phase commit protocol to gurantee strong consistency.
Each participant has to agree in order to successfully complete a transaction.
In a mobile setting with long periods of disconnection each commit could take hours or days to be acknowledged by all nodes.
This is further complicated as nodes are often not fixed and can be added or removed from a mobile application at any time.


In a distributed database this property can only 

TODO:

- master-less database, no global coordinater

- offline by default --> no globally serializable commits, two-phase commit no suitable

- atomicity (all or nothing commit) only on local nodes

- only eventual consistency

- isolation: has to be relaxed as we cannot allow locking --> concurrent transactions cannot be serialized, use lowest level of isolation ("Read uncommitted", concurrent commits to local databases will eventually be consistent

- durability: only local durability, global durability can not be guaranteed in a master-less database

\section{Stream-Based Synchronization}
\label{sec:background.stream-based}
An application that tracks each edit and sends it in a stream to remote nodes follows a stream-based synchronization protocol.
Stream-based synchronization is very common among real-time document editors like Google Docs.\\

An edit usually represents an insert or delete operation at a certain position in the text.
These edit operations are broadcast to remote nodes and then ``replayed''.
As participating nodes can concurrently edit a document the stream of edit operations can not just be applied without modifications.\\
The combination of local modifications and received edit operations from a remote node requires the transformation of the remote operations in order to be correctly applied.\\
The family of algorithms developed to correctly transform the edit operations is described as \emph{Operational Transformation} \cite{Ellis:1998vf}.\\
If some nodes are temporarily offline while continuing to edit, the correct transformation of many concurrent edit operations becomes very complex and error-prone.\\
A practical problem in modern user interfaces is that it is hard to correctly capture all edits made to data.
If a single edit is missed the result is a fork possibly rendering all future update operations as incorrect.
Packet loss due to unreliable network connections have also be taken into account which further complicates the design of a robust algorithm.\\
Research has therefore investigated options for data synchronization that do not require Operational Transformation.\\

\emph{Commutative Replicated Data Types} (CRDTs) have emerged as a viable alternative for specific use cases.
A recent study by Shapiro et al. presents a range of data types designed for synchronization without concurrency control \cite{Shapiro:2011wy}.\\
CRDTs are designed in a way that all edit operations commute when applied in \emph{causal order}.
Section \ref{sec:main.requirements.causality} goes into more detail about causal ordering of events.
Due to the restrictions on supported operations on data types, CRDTs are only applicable in a narrow set of scenarios.

TODO:

- more details about OT and CRDTs

\section{History-Based Synchronization}
\label{sec:background.history-based}
Snapshot-based methods work by tracking and relating an application's data state over time.
Instead of sending a sequential stream of raw updates, each client collects additional metadata that allows more complex reasoning about the state of each client.\\
A prominent example is the distributed content tracking system \emph{git} \cite{git} which can resolve the most complex peer-to-peer synchronization scenarios.\\
Git achieves this by storing the entire history of a project's database on each client.
Each edit made to objects in the database is stored as a commit object and related to its ancestors.\\
Through the resulting commit graph each client can identify the exact subset of updates each remote node has to receive in order to be in sync.\\
While it sounds extremely inefficient to store the entire history of a database, git manages to do this in a very efficient way through a \emph{Content Addressable Store} and data compression.
It is not uncommon that the uncompressed form of the current state of a git project is larger than the project's entire history.

\section{Three-Way Merging}
\label{sec:background.merging}
Three-way merging describes the concept for an algorithm that performs a merge operation on two objects based on a common ancestor.\\
Let \emph{A} be the initial state of the object and let \emph{B} and \emph{C} be edited versions of \emph{A}.
The goal is to merge \emph{B} and \emph{C} into a new object \emph{D}.\\
The merge algorithm starts by identifying the differences between \emph{A} and \emph{B} and between \emph{A} and \emph{C}.\\
All \emph{parts} of object \emph{B} that are neither changed in \emph{B} nor in \emph{C} are carried over into \emph{D}.\\
All changes to parts of the object in \emph{B} that have not been changed in \emph{C} are directly accepted and added to \emph{D}.\\
If the same parts are edited both in \emph{B} and \emph{C} we have a merge conflict that needs to be resolved.\\
There is no universal algorithm for resolving conflicts.
Different types of data and applications require different types of conflict resolution strategies.
In many cases conflict resolution can not even be done in an automated way but has to be left to the user of an application.\\
Even the term \emph{three-way merging} only describes a general concept but the actual algorithm will differ based on the type of objects that are merged.
Text files are the most common type of object with lines seen as the \emph{parts}.
The unix program \emph{diff3} implements a three-way merge variant for text files \cite{diff3}.\\
Most modern version control systems implement three-way merging to allow lock-free collaboration on source code.
\emph{Git} applies three-way merging not only for text files but for entire file system trees \cite{git}.\\
With git we have a great example of a hierarchical conflict resolution strategy:

\begin{itemize}
\item If two developers concurrently edit the same directory git tries to resolve this conflict by descending into the directory and looking at individual files.
\item If the developers edited different files git can automatically resolve the conflict by accepting both changes.
\item If the same file was edited concurrently git tries to descend a level deeper by looking at edits made to individual lines.
\item If different lines were edited concurrently it can again resolve the conflict by accepting both changes.
\item Only in the unlikely event that both developers edited the same line git has no way to automatically resolve the conflict.
It will delegate the conflict resolution to the developers who will have to manually merge both changes.
\end{itemize}

Tancred Lindholm designed a three-way merging algorithm for XML-documents.
With the \emph{3DM} tool there is even an implementation available \cite{Lindholm:2001uv}.
As XML supports the expression of a broad range of data types this is probably one of the most generic implementations.

\section{Most Recent Common Ancestor}
\label{background.mrca}
- describe problem with graphs

- describe solution referring to standard algo

\section{Content Adressable Storage}
- copy on write

- simple verification of data, free checksums

- git as example

\section{HTML5 and Offline Applications}
HTML5 specifies a number of client-side storage options. Most are a work in process and still have to be adopted by all browser vendors. IndexedDB is most likely going to be the standard for building offline-capable web applications. Combined with Cache Manifests, HTML5 provides all the tools necessary for building offline applications.

\subsection{Web Storage}
The simplest API is the \emph{localStorage} standard defined in the W3C's Web Storage specification \cite{webstorage}.\\
It provides a key-value store accessible from JavaScript which can store string values for string keys.
Most browsers currently set a storage limit of 5 MB per site.
\emph{LocalStorage} is therefore only suitable for storing small volumes of data.\\
Another limitation is the interface which is synchronous. As JavaScript is single-threaded, every read or write operation will block the entire application.
Frequent or large-volume read/write operations can result in a bad user experience caused by a ``freezing'' user-interface.\\
\emph{LocalStorage} is currently supported by all major browsers including its mobile variants.

\subsection{Web SQL Database}
A much more advanced implementation is specified by the now deprecated \emph{Web SQL} standard \cite{websql}. It defines a relational database similar to Sqlite including SQL support.\\
The proposal was strongly opposed by the Mozilla Foundation who sees a SQL-based database as a bad fit for web applications \cite{mozilla_indexeddb}.\\
The standard was therefore only implemented by Google Chrome, Safari and Opera and their mobile counterparts in Android and iOS.\\
\emph{Web SQL} has been officially deprecated by the W3C and support by browsers is likely going to drop in the future.

\subsection{Indexed Database}
Instead of Web SQL the standard favored by the W3C and most browser vendors is \emph{IndexedDB}.\\
\emph{IndexedDB} defines a lower-level interface for storing key/value pairs and setting up custom indexes.
While relatively simple, the API design is generic enough to cater for implementations of more complex databases on top.
It would, for example, be possible to implement a \emph{Web SQL} database using \emph{IndexedDB}.\\
IndexedDB supports storing large amounts of data and defines an asynchronous API.\\
Unfortunately the standard has not yet been implemented across all major browsers.
It is currently available in Mozilla Firefox, Google Chrome and Internet Explorer.
Safari support is still missing as well as support in the default Android and iOS browser.\\
Luckily most browsers who have not implemented IndexedDB yet, are still supporting Web SQL.
There is a polyfill available that implements an IndexedDB interface using Web SQL \cite{indexeddb_polyfill}. Application developers can therefore already base their work on the IndexedDB interface while browser vendors are catching up.

\subsection{Cache Manifests}
To truely work offline, an application has to make its static resources available locally as well.
The \emph{cache manifest} defined in the HTML standard gives developers the right tool \cite{cache_manifests}. It allows you to define a local cache of all application resources like HTML, CSS, JavaScript code or other static files.\\
Flexible policies give fine-grained control over which resources should be available offline and which need network connection.
