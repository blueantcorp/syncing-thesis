
\section{Architecture of Histo}
\label{sec:main.histo}

Based on the requirements and the evaluation of CouchDB we derive a new architecture for a practical synchronization solution.

\begin{itemize}
\item
  \textbf{No Timestamps}: history-based 3-way merging
\item
  \textbf{No Change Tracing}: explicit change tracing is not necessary - support
  diff computation on the fly
\item
  \textbf{Data Agnostic}: leave diff and merge of the actual data to
  plugins
\item
  \textbf{Distributed}: synchronization does not require a central server
\item
  \textbf{Functional Design}: only implement the functional parts of synchronization - leave everything else to the application (transport, persistence)
\item
  \textbf{Sensitive Defaults}: have defaults that `just work' but
  still support custom logic (e.g. for conflict resolution)
\item
  \textbf{Cross-Platform}: be available on every major platform through the use of Web Standards
\end{itemize}

\section{Merging of Models}
In this section we will focus on the core merging semantics leaving out details about update propagation which will be discussed later in section \ref{sec:main.protocol}.\\
As described in section \ref{sec:background.definition} synchronization always starts with an update detection phase.
Before we can start to merge branches we need to know about what has changed.
One our our design goals is to relieve the application developer from manual change tracing.
We therefore need to implement a differencing algorithm for the types of models described in section \ref{sec:main.requirements.data-models}.\\
The meta model definition in the form of entities, their attributes and relationships is usually static and does not have to be merged.
We have to focus on merging the actual data which is structured through entity instances, their attribute values and relationships to other instances.
Most modern web application frameworks realize one-to-one relationships by simply having an attribute storing the related instance's ID.
One-to-many relationships are realized through an attribute having a collection of 
instance IDs.\\
Instance collections often have a relevant order that needs to be preserved.
An example is the list of tasks in a project that is displayed to the user.
The user wants to be able to change the order of tasks and the order should be persisted.\\
Summarizing the kind of structures we need to synchronize:

\begin{itemize}
\item Instances with their attribute values
\item Attribute values that can be literals or instance IDs (modeling one-to-one relations)
\item Attribute values that can be ordered collections (modeling one-to-many relations)
\end{itemize}

We have described three-way merging semantics in section \ref{sec:background.merging}.
Our algorithm will be structured into three distinct phases.
Starting with a differencing phase we identify the changes made in two branches of our state B and C since our common ancestor state A.
Should an application decide to explicitly track changes it could actually leave this phase out and still benefit from the rest of our merging algorithm.\\
If follows a diff merging phase where the two diff results A-B and A-C are combined into one diff.
As its only input is the two diffs it does not require access to the actual states A, B or C.
In this phase we can have conflicts if the two branches contain updates to the same parts of the state.\\
The merged diff can then be applied to the origin state A in order to create the actual merged state.
For this step we require a patch algorithm.

Let us summarize the tree steps of the complete merge process of two branches:

\begin{itemize}
\item \textbf{Differencing}: we diff branch B and C with their common ancestor A.
\item \textbf{Merging}: we merge the two diffs A-B and A-C to a new diff result.
\item \textbf{Patching}: we apply the merged diffs as a patch to A which results in the merged state D.
\end{itemize}

\subsection{Differencing Test Case}
To show correctness of a differencing algorithm we need to define sample model states with expected differencing results.
Based on an ancestor state all users start with we define several possible branch states.
For each branch state we will define the difference to the ancestor state.
This set of data can then be used as a test case for our implementation.\\

\begin{tabular}{ l l l l }
\multicolumn{4}{ c }{\textbf{Ancestor state A of Projects}} \\
ID & Project Name & Members & Tasks \\
\hline
1 & Marketng Material & Rita, Tom, Allen & 1, 2, 3, 4 \\
2 & Product Roadmap & Rita, Allen & 5 \\
\end{tabular} \\

`Members' is an unordered collection of users - we define the user's IDs identical to their names.\\
`Tasks' is an ordered collection of tasks IDs.\\
We will leave out the details about other entities as the project instances already cover all required modeling aspects.\\

\begin{tabular}{ l l l l }
\multicolumn{4}{ c }{\textbf{State B of Projects}} \\
ID & Project Name & Members & Tasks \\
\hline
1 & Marketing Material & Rita, Tom & 1, 4, 2, 3, 6 \\
2 & Product Planning & Rita, Allen & 5 \\
\end{tabular} \\
\\

\begin{tabular}{ l l l l }
\multicolumn{4}{ c }{\textbf{State C of Projects}} \\
ID & Project Name & Members & Tasks \\
\hline
1 & Marketing Strategy & Rita, Tom, Allen & 4, 1, 2, 3 \\
2 & Product Strategy & Rita, Allen & 5, 7 \\
\end{tabular} \\
\\

The respective differencing outputs of A-B and A-C are defined as the following.
Note that all index positions are seen as relative to the origin state.
So even if one diff contains multiple insert, move or remove operations they are given as if they were all applied simultaneously to the origin state.\\

\begin{tabularx}{\textwidth}{ l X l X }
\multicolumn{4}{ c }{\textbf{Expected diff A-B}} \\
ID & Project Name & Members & Tasks \\
\hline
1 & insert `i' behind index 5 & remove `Allen' & move index 3 behind index 0 \newline insert 6 behind index 3 \\
2 & remove from index 8 to 15 \newline insert `Planning' behind index 7 & unchanged & unchanged \\
\end{tabularx} \\
\\

\begin{tabularx}{\textwidth}{ l X l X }
\multicolumn{4}{ c }{\textbf{Expected diff A-C}:} \\
ID & Project Name & Members & Tasks \\
\hline
1 & insert `i' behind index 5 \newline remove from index 9 to 17 \newline insert `Strategy' behind index 8 & unchanged & move index 3 behind index -1 \\
2 & remove from index 8 to 15 \newline insert `Strategy' behind index 7 & unchanged & insert 7 behind index 0 \\
\end{tabularx} \\

In this differencing output we actually do not treat the project name as an atom but actually go a level deeper and difference its characters.\\
Depending on the application this might not be necessary.
A simpler output which treats the project titles as atoms would look like this:\\

\begin{tabular}{ l l l l }
\multicolumn{2}{ c }{\textbf{Simpler `Project Name' diff A-B}} \\
ID & Project Name \\
\hline
1 & change to `Marketing Material' \\
2 & change to `Product Planning' \\
\end{tabular} \\
\\

\begin{tabular}{ l l l l }
\multicolumn{2}{ c }{\textbf{Simpler `Project Name' diff A-C}} \\
ID & Project Name \\
\hline
1 & change to `Marketing Strategy' \\
2 & change to `Product Strategy' \\
\end{tabular} \\
\\

The choice of differencing granularity will affect what we will see as conflicts in the merging stage.

\subsection{Differencing Algorithm}
Differencing algorithms have been studied extensively.
There exist efficient solutions for a range of data structures.
It is not a focus of our thesis to develop the most efficient differencing algorithm matching our scenario.
Our goal in this section is to show the practical feasibility of the three-way merging component in our architecture.
We favor a simple solution, re-using existing concepts so that we can broaden our focus on an other areas of our synchronization solution.\\
The test case defined in the previous section can be decomposed and mapped to common data structures.\\
An instance, in our example a project, actually represents a \emph{dictionary} data structure or a set of key-value entries.
The dictionary keys are mapped to the attribute names and the dictionary values to the respective attribute value.\\
Attribute values that represent collections of instance IDs can be mapped to \emph{sets}.
Our `Members' attribute does not have a relevant order, it can be mapped to a normal set.
The `Tasks' attribute has a significant order and therefore has to be mapped to an \emph{ordered set}.\\
If we do not treat string values as atoms we have to represent them as an \emph{ordered list}.\\

To summarize - for the following data structures we need an algorithm that finds the difference from an origin state A to a changed state B:

\begin{itemize}
\item \textbf{Dictionaries}: to model entity instances.
\item \textbf{Sets and Ordered Sets}: to model instance relationships.
\item \textbf{Ordered Lists / Strings}: to model string values of instance attributes.
\end{itemize}

Ordered list or string data structures have been the main focus in previous research on difference algorithms.
Myers presented an efficient algorithm with $\mathcal O(n*d) $ time and space complexity to difference two strings A and B with
$ n $ representing the sum of the lengths of two strings and $ d $ the size of the shortest edit script transforming A to B \cite{Myers:1986wi}.
The shortest edit script is equivalent to the result of a differencing algorithm.
As in practical applications differences are usually small the algorithm performs well.\\
Taking the ordered list difference algorithm as given, it can easily be re-used to build an algorithm for ordered sets.
Ordered lists can only have differences in the form of \emph{insert} or \emph{remove} operations.
Ordered sets extend this - the simultaneous remove and insert of a globally unique element is now considered as a move operation.\\

To implement an ordered set algorithm we can therefore take the output of the ordere list algorithm and scan the result for the remove and insert of the same element.
This can be efficiently implemented through a hash:

\begin{enumerate}
\item Scan the diff result and build a hash for all removed elements.
\item Scan the result again and test for all inserted elements whether they are included in the hash.
\item If a match is found replace the remove and insert operations through a single move operation in the result.
\end{enumerate}

The time complexity of building a hash can be estimated with $ \mathcal O(n * log(n)) $ with $ n $ representing set size.
The match searching has linear time complexity.
We therefore only add $ \mathcal O(n * log(n)) $ complexity to Myers difference algorithm for a naive solution for ordered sets.\\

For simple sets we can implement a naive solution through two hashs of the respective set entries combined with a scan through each set:

\begin{enumerate}
\item Add all entries of set A to a hash $ H_A $ and those of set B to a different hash $ H_B $.
\item Scan set A and test for matches in hash $ H_B $.
\item If no match is found add a remove operation to the result.
\item Scan set B and test for matches in hash $ H_A $.
\item If no match is found add an insert operation to the result.
\end{enumerate}

\emph{Insert} and \emph{remove} are the only operations in a set.\\
The time complexity for building each hash is again $ \mathcal O(n * log(n)) $.
Searching for matches has linear complexity which results in a time complexity $ \mathcal O(n * log(n)) $ for the entire algorithm.\\

A dictionary data structure has \emph{insert}, \emph{remove} and \emph{update} operations.
If the instance described through the dictionary has a fixed set of attributes it would actually only need to support an update operation.
In modern web applications it is not uncommon that there is no fixed data schema.
It is often the case that new attributes are added to instances at runtime.
Even if there is a fixed schema it might be changed through a software update with old instances not being migrated to the new schema.
We should therefore support the full set of dictionary operations in our difference algorithm.

A simple and efficient solution is to again use two hashes for fast lookup combined with a scan through both dictionaries:

\begin{enumerate}
\item All all keys and values of dictionary A to hash $ H_A $ and those of dictionary B to hash $ H_B $.
\item Scan through all key-value entries of A.
\item If the key is not included in hash $ H_B $, add a remove operation to the result.
\item If the key is included and the value in $ H_B $ is different, add an update operation.
\item Scan through all key-value entries of B.
\item If the key is not included in hash $ H_A $, add an insert operation to the result.
\end{enumerate}

Building the hash is again estimated with time complexity $ \mathcal O(n * log(n)) $, scanning both dictionaries has linear time complexity. The combined time complexity is therefore again $ \mathcal O(n * log(n)) $.\\
Depending on the application, its instances are often already implemented with hash-like lookup performance - in this case we could skip step 1.
The time complexity is in this case only linear.\\

Differencing of our entire data is actually a hierachical process combining all of these algorithms.\\
At the highest level we have a set of project IDs where we use our set difference algorithm to identify removed or new projects.
For each project we then need to dive a level deeper and difference the actual instances - here we use the dictionary algorithm.
If our project only has atoms as attribute values the process stops here.
If there are string values we might choose to do finer grained differencing using the respective algorithm.
If we have one-to-many relationships in the instance we have to compute an (ordered) set difference.\\
The same process has to be repeated for each entity's instances.

\subsection{Diff Merging Test Case}

\subsection{Diff Merging Algorithm}

\subsection{Patching}

TODO:

- explain diff, merge and patch

- implement diff, merge and patch logic for primitive data structures
  -> use them to recursively model complex data structures

- ensure conflicts are made explicit

- efficient child tree pointers like in git

- instances are key-value sets

- collections are ordered sets

- take ordered-list diff as granted

\section{Storing and Commiting Changes}
\label{main.committing}
As syncing is state based we need to track the history of edits on each client.\\Each client has his own replica of the database and commits
data locally.\\On every commit we create a commit object that links both
to the new version of the data and the previous commit.\\

TODO:

- use content-adressable store

- only store changes and reference unchanged data through hashs --> like git

- commit links to data and parent commit

\section{Differencing Across Commits}
\label{main.diff-across-commits}

TODO:

- Most Recent Common Ancestor algorithm used for finding common commit of clients

- walk commit graph until LCA

- recursive application of LCA on every fork in graph

- implementation as separate module

- use per-commit diff to find full data diff across commits

\section{Synchronization Protocol}
\label{sec:main.protocol}
Synchronization always happens from a \emph{Source} node to a \emph{Target} node.
If it is run simultaneously with Source and Target exchanged, it keeps both nodes in sync with each other.\\
The algorithm is designed to be able to run independently of the Source or Target.
It could be implemented as a separate application possibly even running on a different device - as long as it has access to both the Source and Target node.\\
The Synchronizer could be run in regular intervals or explicitly triggered by changes in the Source node.\\

The latest commit on a node we refer to as the \emph{head}.
A node has a \emph{master head} which refers to the version of the data considered to be `true' by the node.\\
For each remote node it synchronizes with, the node keeps a \emph{remote tracking head}.\\
A remote tracking head represents what the local node considers to be the current state of a remote node.

Synchronization follows a two-step protocol, step one propagates all changed data from Source to Target, step two executes a local merge operation.

\subsubsection{Propagation}
Propagation follows the following protocol:

\begin{enumerate}
\item Read all commit IDs since the last synced commit from Source and write them to Target.
\item Let the Target compute the common ancestor commit ID of Target's and Source's master heads.
\item Read all changed data since the common ancestor commit from Source and write to Target.
\item Set the Target's remote tracking head of Source to Source's master head.
\end{enumerate}

Once these steps are executed, the Target node has the current state of Source available locally.\\
The Target's head still refers to the same state as the Source data has not been merged.\\

Listing \ref{propagation-protocol} summarizes the protocol as pseudo-code.

\begin{lstlisting}[caption=Propagation Protocol, label=propagation-protocol]

commitIDsSource = source.getCommitIDsSince(lastSyncedCommit)

target.writeCommitIDs(commitIDsSource)

commonAncestor = target.getCommonAncestor(target.head.master, source.head.master)

changedData = source.getChangedDataSince(commonAncestor)

target.writeData(changedData)

\end{lstlisting}

The functions `getCommitIDsSince()' and `getChangedDataSince()' are implemented as described in section \ref{main.diff-across-commits}.\\
The most recent common ancestor algorithm used in `getCommonAncestor()' is described in section \ref{background.mrca}.\\
The internals used by `writeData()' and the underlying commit data model are explained in section \ref{main.committing}.

\subsubsection{Merging}
Even if the Source is disconnected at this stage, the Target has all the necessary information to process the merge offline:\\

\begin{itemize}
\item The Target's master head we refer to as the \emph{master head}.\\
\item The Target's remote tracking branch for the Source we refer to as the \emph{Source tracking head}.
\end{itemize}

\begin{enumerate}
\item Compute the common ancestor of the master head and the Source tracking head. (The common ancestor could also be re-used from the propagation step.)
\item If the common ancestor equals the Source tracking head:\\
  The Source has not changed since the last synchronization. The master head is ahead of the Source tracking head.\\
  The algorithm can stop here.
\item If the common ancestor equals the master head:\\
  The Target has not changed since the last synchronization.\\
  The Source's head is ahead of Target.\\
  We can fast-forward the master head to the Source tracking head.
\item If the common ancestor is neither the Source tracking head nor the master head:\\
  Both Source and Target must have changed data since the last synchronization.\\
  We run a three-way merge of the common ancestor, Source tracking head and master head.\\
  We commit the result as the new master head.
\end{enumerate}

This protocol is able to minimize the amount of data sent between synced
stores even in a distributed, peer-to-peer setting.\\

Updating the Target's head uses optimistic locking.
To update the head you need to include the last read head in your request.
So both the fast-forward operation or the commit of a merge result can be rejected if the Target has been updated in the meantime.
If this happens the Synchronizer simply has to re-run the merge algorithm.\\

The merging process can be described in pseudo-code as shown in figure \ref{merging-protocol}.

\begin{lstlisting}[caption=Merging Protocol, label=merging-protocol]

masterHead = target.head.master
sourceTrackingHead = target.head.sourceID

commonAncestor = target.getCommonAncestor(masterHead, sourceTrackingHead)

if (commonAncestor == sourceTrackingHead) {
  // do nothing

} else if (commonAncestor == masterHead) {
  // fast-forward master head
  try {
    // when updating the head we have to pass in the previous head:
    target.setHead(sourceTrackingHead, masterHead)
  } catch {
    // the master head has been updated in the meantime
    // start over
  }

} else {
  commonAncestorData = target.getData(commonAncestor)
  sourceHeadData = target.getData(sourceTrackingHead)
  targetHeadData = target.getData(masterHead)

  mergedData = three-way-merge(commonAncestorData, sourceHeadData, targetHeadData)

  // commit object linking commit data with its ancestors:
  commitObject = createCommit(mergedData, [masterHead, sourceTrackingHead])

  try {
    // when updating the head we have to pass in the previous head:
    target.commit(commitObject, masterHead)    
  } catch {
    // the master head has been updated in the meantime
    // start over
  }
}

\end{lstlisting}

\section{Handling Conflicts}

TODO:

- application specific, no general solution

- automatic resolution strategies

- manual resolution through user

\section{Synchronization Topologies}

TODO:

- document different supported topologies

- client-server

- client-client

- client-server + server-server

- hierarchical (office server + cloud server)

\section{Optimizations}

TODO:

- Only keep limited history.

- Clients who are disconnected for too long have to fetch redundant data.

- Ideal case: remember until oldest head of nodes.

\section{Integration with Application Logic}

TODO:

- demonstrate how to interface with standard MVC frameworks like Backbone, Ember.js, Angular
